{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "77f66dbe-192b-471c-9cb8-e9b365e61bbb",
      "metadata": {
        "id": "77f66dbe-192b-471c-9cb8-e9b365e61bbb"
      },
      "source": [
        "# Lab | Tools prompting\n",
        "\n",
        "**Replace the existing two tools decorators, by creating 3 new ones and adjust the prompts accordingly**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14b94240",
      "metadata": {
        "id": "14b94240"
      },
      "source": [
        "### How to add ad-hoc tool calling capability to LLMs and Chat Models\n",
        "\n",
        ":::{.callout-caution}\n",
        "\n",
        "Some models have been fine-tuned for tool calling and provide a dedicated API for tool calling. Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling. Please see the [how to use a chat model to call tools](https://python.langchain.com/docs/how_to/tool_calling/) guide for more information.\n",
        "\n",
        "In this guide, we'll see how to add **ad-hoc** tool calling support to a chat model. This is an alternative method to invoke tools if you're using a model that does not natively support tool calling.\n",
        "\n",
        "We'll do this by simply writing a prompt that will get the model to invoke the appropriate tools. Here's a diagram of the logic:\n",
        "\n",
        "<br>\n",
        "\n",
        "![chain](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/tool_chain.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a22cb8-19e7-450a-9d1b-6848d2c81cd1",
      "metadata": {
        "id": "a0a22cb8-19e7-450a-9d1b-6848d2c81cd1"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We'll need to install the following packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8c556c5e-b785-428b-8e7d-efd34a2a1adb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c556c5e-b785-428b-8e7d-efd34a2a1adb",
        "outputId": "efaedf30-4d0b-4f13-ad20-9c719639a885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m434.1/434.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet langchain langchain-community langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "897bc01e-cc2b-4400-8a64-db4aa56085d3",
      "metadata": {
        "id": "897bc01e-cc2b-4400-8a64-db4aa56085d3"
      },
      "source": [
        "If you'd like to use LangSmith, uncomment the below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5efb4170-b95b-4d29-8f57-09509f3ba6df",
      "metadata": {
        "id": "5efb4170-b95b-4d29-8f57-09509f3ba6df"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ec6409b-21e5-4d0a-8a46-c4ef0b055dd3",
      "metadata": {
        "id": "7ec6409b-21e5-4d0a-8a46-c4ef0b055dd3"
      },
      "source": [
        "You can select any of the given models for this how-to guide. Keep in mind that most of these models already [support native tool calling](https://python.langchain.com/docs/integrations/chat), so using the prompting strategy shown here doesn't make sense for these models, and instead you should follow the [how to use a chat model to call tools](https://python.langchain.com/docs/how_to/tool_calling/) guide.\n",
        "\n",
        "```{=mdx}\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs openaiParams={`model=\"gpt-4\"`} />\n",
        "```\n",
        "\n",
        "To illustrate the idea, we'll use `phi3` via Ollama, which does **NOT** have native support for tool calling. If you'd like to use `Ollama` as well follow [these instructions](https://python.langchain.com/docs/integrations/chat/ollama)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "424be968-2806-4d1a-a6aa-5499ae20fac5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "424be968-2806-4d1a-a6aa-5499ae20fac5",
        "outputId": "00d92d89-0807-4339-fcb4-42d1d6fe4810"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-c8dd050808eb>:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
            "  model = Ollama(model=\"phi3\")\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import Ollama\n",
        "\n",
        "model = Ollama(model=\"phi3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a88a1463",
      "metadata": {
        "id": "a88a1463"
      },
      "source": [
        "\n",
        "#  How to Install and Run Ollama with the Phi-3 Model\n",
        "\n",
        "This guide walks you through installing **Ollama** and running the **Phi-3** model on Windows, macOS, and Linux.\n",
        "\n",
        "---\n",
        "\n",
        "## Windows\n",
        "\n",
        "1. **Download Ollama for Windows**  \n",
        "   Go to: [https://ollama.com/download](https://ollama.com/download)  \n",
        "   Download and run the installer.\n",
        "\n",
        "2. **Verify Installation**  \n",
        "   Open **Command Prompt** and type:\n",
        "   ```bash\n",
        "   ollama --version\n",
        "   ```\n",
        "\n",
        "3. **Run the Phi-3 Model**  \n",
        "   In the same terminal:\n",
        "   ```bash\n",
        "   ollama run phi3\n",
        "   ```\n",
        "\n",
        "4. **If you get a CUDA error (GPU memory issue)**  \n",
        "   Run Ollama in **CPU mode**:\n",
        "   ```bash\n",
        "   set OLLAMA_NO_CUDA=1\n",
        "   ollama run phi3\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "##  macOS\n",
        "\n",
        "1. **Install via Homebrew**  \n",
        "   Open the Terminal and run:\n",
        "   ```bash\n",
        "   brew install ollama\n",
        "   ```\n",
        "\n",
        "2. **Run the Phi-3 Model**\n",
        "   ```bash\n",
        "   ollama run phi3\n",
        "   ```\n",
        "\n",
        "3. **To force CPU mode (no GPU)**\n",
        "   ```bash\n",
        "   export OLLAMA_NO_CUDA=1\n",
        "   ollama run phi3\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "##  Linux\n",
        "\n",
        "1. **Install Ollama**  \n",
        "   Open a terminal and run:\n",
        "   ```bash\n",
        "   curl -fsSL https://ollama.com/install.sh | sh\n",
        "   ```\n",
        "\n",
        "2. **Run the Phi-3 Model**\n",
        "   ```bash\n",
        "   ollama run phi3\n",
        "   ```\n",
        "\n",
        "3. **To force CPU mode (no GPU)**\n",
        "   ```bash\n",
        "   export OLLAMA_NO_CUDA=1\n",
        "   ollama run phi3\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "##  Notes\n",
        "\n",
        "- The first time you run `ollama run phi3`, it will **download the model**, so make sure youâ€™re connected to the internet.\n",
        "- Once downloaded, it works **offline**.\n",
        "- Keep the terminal open and running in the background while using Ollama from your code or notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68946881",
      "metadata": {
        "id": "68946881"
      },
      "source": [
        "## Create a tool\n",
        "\n",
        "First, let's create an `add` and `multiply` tools. For more information on creating custom tools, please see [this guide](https://python.langchain.com/docs/how_to/custom_tools/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4548e6fa-0f9b-4d7a-8fa5-66cec0350e5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4548e6fa-0f9b-4d7a-8fa5-66cec0350e5f",
        "outputId": "2dccaa8a-fad3-41c9-fd2f-8249dceea197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--\n",
            "multiply\n",
            "Multiply two numbers together.\n",
            "{'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}\n",
            "--\n",
            "add\n",
            "Add two numbers.\n",
            "{'x': {'title': 'X', 'type': 'integer'}, 'y': {'title': 'Y', 'type': 'integer'}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiply two numbers together.\"\"\"\n",
        "    return x * y\n",
        "\n",
        "\n",
        "@tool\n",
        "def add(x: int, y: int) -> int:\n",
        "    \"Add two numbers.\"\n",
        "    return x + y\n",
        "\n",
        "\n",
        "tools = [multiply, add]\n",
        "\n",
        "# Let's inspect the tools\n",
        "for t in tools:\n",
        "    print(\"--\")\n",
        "    print(t.name)\n",
        "    print(t.description)\n",
        "    print(t.args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "be77e780",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be77e780",
        "outputId": "718ff84e-16c6-4f68-d263-7dfd3393c67f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20.0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multiply.invoke({\"x\": 4, \"y\": 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15dd690e-e54d-4209-91a4-181f69a452ac",
      "metadata": {
        "id": "15dd690e-e54d-4209-91a4-181f69a452ac"
      },
      "source": [
        "## Creating our prompt\n",
        "\n",
        "We'll want to write a prompt that specifies the tools the model has access to, the arguments to those tools, and the desired output format of the model. In this case we'll instruct it to output a JSON blob of the form `{\"name\": \"...\", \"arguments\": {...}}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2063b564-25ca-4729-a45f-ba4633175b04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2063b564-25ca-4729-a45f-ba4633175b04",
        "outputId": "1bd25b35-b47b-4a6f-940b-1f5444ae7d39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "multiply(x: float, y: float) -> float - Multiply two numbers together.\n",
            "add(x: int, y: int) -> int - Add two numbers.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.tools import render_text_description\n",
        "\n",
        "rendered_tools = render_text_description(tools)\n",
        "print(rendered_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f02f1dce-76e7-4ca9-9bac-5af496131fe1",
      "metadata": {
        "id": "f02f1dce-76e7-4ca9-9bac-5af496131fe1"
      },
      "outputs": [],
      "source": [
        "system_prompt = f\"\"\"\\\n",
        "You are an assistant that has access to the following set of tools.\n",
        "Here are the names and descriptions for each tool:\n",
        "\n",
        "{rendered_tools}\n",
        "\n",
        "Given the user input, return the name and input of the tool to use.\n",
        "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
        "\n",
        "The `arguments` should be a dictionary, with keys corresponding\n",
        "to the argument names and the values corresponding to the requested values.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ueUj0LpeYQMG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueUj0LpeYQMG",
        "outputId": "82f0b7aa-377d-4eb5-bc93-7ccd6461a94a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XuWMft9QYje1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuWMft9QYje1",
        "outputId": "291a1fe9-7212-4446-fc2f-c4bff1bf2105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.55)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-5ef526745e65>:15: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1135\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install openai langchain\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"my key\"  # Replace with your actual key\n",
        "\n",
        "# Create the prompt template\n",
        "prompt = PromptTemplate.from_template(\"What's {input}?\")\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Create the chain by combining prompt and model\n",
        "chain = prompt | model\n",
        "\n",
        "# Invoke the chain with input\n",
        "message = chain.invoke({\"input\": \"3 plus 1132\"})\n",
        "\n",
        "# Print the result depending on output type\n",
        "if isinstance(message, str):\n",
        "    print(message)\n",
        "else:\n",
        "    print(message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14df2cd5-b6fa-4b10-892d-e8692c7931e5",
      "metadata": {
        "id": "14df2cd5-b6fa-4b10-892d-e8692c7931e5"
      },
      "source": [
        "## Adding an output parser\n",
        "\n",
        "We'll use the `JsonOutputParser` for parsing our models output to JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f129f5bd-127c-4c95-8f34-8f437da7ca8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f129f5bd-127c-4c95-8f34-8f437da7ca8f",
        "outputId": "104b550f-dc9f-4c9e-8ed6-84a550c34cb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (0.3.23)\n",
            "Requirement already satisfied: openai in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (1.75.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langchain) (0.3.52)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langchain) (0.3.30)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: certifi in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (8.2.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\shathakamal88\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (2.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\shathakamal88\\AppData\\Local\\Temp\\ipykernel_20800\\3772670823.py:18: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n"
          ]
        },
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for ChatOpenAI\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'model_kwargs': {}, 'nam...ne, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.8/v/value_error",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer the following question and return only a JSON object in the format: \u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m: <your answer>}}\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize the OpenAI model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Initialize the JSON parser\u001b[39;00m\n\u001b[0;32m     21\u001b[0m parser \u001b[38;5;241m=\u001b[39m JsonOutputParser()\n",
            "File \u001b[1;32mc:\\Users\\shathakamal88\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:224\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     emit_warning()\n\u001b[1;32m--> 224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\shathakamal88\\anaconda3\\Lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\shathakamal88\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:193\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    192\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(data, self_instance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'model_kwargs': {}, 'nam...ne, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.8/v/value_error"
          ]
        }
      ],
      "source": [
        "# Install required packages (if not already installed)\n",
        "!pip install langchain openai\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "import os\n",
        "\n",
        "# # Set your OpenAI API key\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass(\"ğŸ” Enter your OpenAI API key: \")\n",
        "\n",
        "# Define prompt with JSON instruction\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"Answer the following question and return only a JSON object in the format: {{\\\"answer\\\": <your answer>}}\\n\\nQuestion: {input}\"\n",
        ")\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Initialize the JSON parser\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "# Chain: prompt -> model -> JSON parser\n",
        "chain = prompt | model | parser\n",
        "\n",
        "# Invoke the chain\n",
        "output = chain.invoke({\"input\": \"what's thirteen times 4\"})\n",
        "\n",
        "# Print the output\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1f08255-f146-4f4a-be43-5c21c1d3ae83",
      "metadata": {
        "id": "e1f08255-f146-4f4a-be43-5c21c1d3ae83"
      },
      "source": [
        ":::{.callout-important}\n",
        "\n",
        "ğŸ‰ Amazing! ğŸ‰ We now instructed our model on how to **request** that a tool be invoked.\n",
        "\n",
        "Now, let's create some logic to actually run the tool!\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e29dd4c-8eb5-457f-92d1-8add076404dc",
      "metadata": {
        "id": "8e29dd4c-8eb5-457f-92d1-8add076404dc"
      },
      "source": [
        "## Invoking the tool ğŸƒ\n",
        "\n",
        "Now that the model can request that a tool be invoked, we need to write a function that can actually invoke\n",
        "the tool.\n",
        "\n",
        "The function will select the appropriate tool by name, and pass to it the arguments chosen by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "faee95e0-4095-4310-991f-9e9465c6738e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faee95e0-4095-4310-991f-9e9465c6738e",
        "outputId": "0e801196-fb1e-4e79-c059-cac524638a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: 28\n"
          ]
        }
      ],
      "source": [
        "from typing import Any, Dict, Optional, TypedDict\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain.tools import tool\n",
        "\n",
        "# âœ… Step 1: Define a sample tool (e.g., multiply)\n",
        "@tool\n",
        "def multiply(x: int, y: int) -> int:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return x * y\n",
        "\n",
        "# âœ… Step 2: Define the tools list\n",
        "tools = [multiply]\n",
        "\n",
        "# âœ… Step 3: Define the ToolCallRequest type\n",
        "class ToolCallRequest(TypedDict):\n",
        "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
        "    name: str\n",
        "    arguments: Dict[str, Any]\n",
        "\n",
        "# âœ… Step 4: Define the invoke_tool function\n",
        "def invoke_tool(\n",
        "    tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None\n",
        "):\n",
        "    \"\"\"A function to perform a tool invocation.\"\"\"\n",
        "    tool_name_to_tool = {tool.name: tool for tool in tools}\n",
        "    name = tool_call_request[\"name\"]\n",
        "    requested_tool = tool_name_to_tool[name]\n",
        "    return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)\n",
        "\n",
        "# âœ… Step 5: Example usage\n",
        "result = invoke_tool({\n",
        "    \"name\": \"multiply\",\n",
        "    \"arguments\": {\n",
        "        \"x\": 7,\n",
        "        \"y\": 4\n",
        "    }\n",
        "})\n",
        "\n",
        "print(\"Result:\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4957532-9e0c-47f6-bb62-0fd789ac1d3e",
      "metadata": {
        "id": "f4957532-9e0c-47f6-bb62-0fd789ac1d3e"
      },
      "source": [
        "Let's test this out ğŸ§ª!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d0ea3b2a-8fb2-4016-83c8-a5d3e78fedbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0ea3b2a-8fb2-4016-83c8-a5d3e78fedbc",
        "outputId": "94ef1018-60d5-4c3b-871a-ffd69491bc26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15.0"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "invoke_tool({\"name\": \"multiply\", \"arguments\": {\"x\": 3, \"y\": 5}})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "715af6e1-935d-4bc0-a3d2-646ecf8a329b",
      "metadata": {
        "id": "715af6e1-935d-4bc0-a3d2-646ecf8a329b"
      },
      "source": [
        "## Let's put it together\n",
        "\n",
        "Let's put it together into a chain that creates a calculator with add and multiplication capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "QR6fCDwrcJVv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR6fCDwrcJVv",
        "outputId": "3015d76f-8c3a-4772-871c-0bc4bde94bf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'answer': 53.73788653}\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    \"Answer this question as a JSON object in the format: {{\\\"answer\\\": <value>}} \\n\\nQuestion: {input}\"\n",
        ")\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "chain = prompt | model | parser\n",
        "\n",
        "output = chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a9c5aa-f60a-4017-af6f-1ff6e04bfb61",
      "metadata": {
        "id": "b4a9c5aa-f60a-4017-af6f-1ff6e04bfb61"
      },
      "source": [
        "## Returning tool inputs\n",
        "\n",
        "It can be helpful to return not only tool outputs but also tool inputs. We can easily do this with LCEL by `RunnablePassthrough.assign`-ing the tool output. This will take whatever the input is to the RunnablePassrthrough components (assumed to be a dictionary) and add a key to it while still passing through everything that's currently in the input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45404406-859d-4caa-8b9d-5838162c80a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45404406-859d-4caa-8b9d-5838162c80a0",
        "outputId": "bdb04170-3d19-42fd-b250-9b6f20fb22b7"
      },
      "outputs": [],
      "source": [
        "# âœ… Step 1: Install necessary packages\n",
        "!pip install -q langchain openai\n",
        "\n",
        "# âœ… Step 2: Import libraries\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.tools import tool\n",
        "import os\n",
        "from getpass import getpass\n",
        "from typing import Any, Dict, Optional, TypedDict\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "# # âœ… Step 3: Enter your OpenAI API key securely\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass(\"ğŸ” Enter your OpenAI API key: \")\n",
        "\n",
        "# âœ… Step 4: Define the tool\n",
        "@tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return x * y\n",
        "\n",
        "tools = [multiply]\n",
        "\n",
        "# âœ… Step 5: ToolCallRequest definition\n",
        "class ToolCallRequest(TypedDict):\n",
        "    name: str\n",
        "    arguments: Dict[str, Any]\n",
        "\n",
        "# âœ… Step 6: invoke_tool function\n",
        "def invoke_tool(\n",
        "    tool_call_request: ToolCallRequest,\n",
        "    config: Optional[RunnableConfig] = None\n",
        "):\n",
        "    tool_name_to_tool = {tool.name: tool for tool in tools}\n",
        "    name = tool_call_request[\"name\"]\n",
        "    requested_tool = tool_name_to_tool[name]\n",
        "    return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)\n",
        "\n",
        "# âœ… Step 7: Prompt\n",
        "prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are a smart assistant. Based on the user's request, return a JSON object with:\n",
        "- \"name\": the tool to call (e.g., \"multiply\")\n",
        "- \"arguments\": a dictionary matching the tool's parameters exactly.\n",
        "\n",
        "Tool: multiply(x: float, y: float)\n",
        "\n",
        "Only return valid JSON. No extra text.\n",
        "\n",
        "User request: {input}\n",
        "\"\"\")\n",
        "\n",
        "# âœ… Step 8: Build the chain\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "chain = (\n",
        "    prompt\n",
        "    | model\n",
        "    | parser\n",
        "    | RunnablePassthrough.assign(output=invoke_tool)\n",
        ")\n",
        "\n",
        "# âœ… Step 9: Run the chain\n",
        "result = chain.invoke({\"input\": \"multiply 13 by 4.14137281\"})\n",
        "print(result)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1797fe82-ea35-4cba-834a-1caf9740d184",
      "metadata": {
        "id": "1797fe82-ea35-4cba-834a-1caf9740d184"
      },
      "source": [
        "## What's next?\n",
        "\n",
        "This how-to guide shows the \"happy path\" when the model correctly outputs all the required tool information.\n",
        "\n",
        "In reality, if you're using more complex tools, you will start encountering errors from the model, especially for models that have not been fine tuned for tool calling and for less capable models.\n",
        "\n",
        "You will need to be prepared to add strategies to improve the output from the model; e.g.,\n",
        "\n",
        "1. Provide few shot examples.\n",
        "2. Add error handling (e.g., catch the exception and feed it back to the LLM to ask it to correct its previous output)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unZ7t_KCgQXa",
      "metadata": {
        "id": "unZ7t_KCgQXa"
      },
      "source": [
        "Examples:\n",
        "User request: multiply 3 by 5  \n",
        "â†’ {\"name\": \"multiply\", \"arguments\": {\"x\": 3, \"y\": 5}}\n",
        "\n",
        "User request: multiply 8.2 and 2  \n",
        "â†’ {\"name\": \"multiply\", \"arguments\": {\"x\": 8.2, \"y\": 2}}\n",
        "\n",
        "User request: {input}\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
